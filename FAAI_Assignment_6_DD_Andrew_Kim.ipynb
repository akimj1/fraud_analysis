{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2160a351-5645-413e-9da9-2d9cc6477b65",
   "metadata": {
    "id": "2160a351-5645-413e-9da9-2d9cc6477b65"
   },
   "source": [
    "# 🧠 Assignment #6: Deepfake Detection Using AI/ML\n",
    "\n",
    "## 🎯 Objective\n",
    "\n",
    "Your task is to detect deepfakes—highly realistic synthetic images generated by models like StyleGAN—using machine learning. You'll apply computer vision techniques and neural networks to classify facial images as either real or fake.\n",
    "\n",
    "You’ll use the dataset from class, which includes labeled image files (real vs. fake) and associated metadata CSVs.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ The Dataset (https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces)\n",
    "\n",
    "The dataset includes:\n",
    "\n",
    "- Folder structure:  \n",
    "  `train/real/`  \n",
    "  `train/fake/`  \n",
    "\n",
    "- Metadata CSVs:  \n",
    "  `train.csv`, `valid.csv`, `test.csv`  \n",
    "\n",
    "Each CSV contains:\n",
    "- `image_id`  \n",
    "- `label` (1 = real, 0 = fake)  \n",
    "- `path` to the image  \n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Your Task\n",
    "\n",
    "Choose **one of the two modeling approaches** below and complete the steps provided.\n",
    "\n",
    "---\n",
    "\n",
    "### Option A: Convolutional Neural Network (CNN)\n",
    "\n",
    "1. Load image metadata and read images using a suitable loader (e.g., `ImageDataGenerator` or `tf.keras.utils.image_dataset_from_directory`).\n",
    "2. Build a basic CNN using Keras or TensorFlow.\n",
    "3. Train the model on the training data and evaluate on the validation/test set.\n",
    "4. Report key metrics: **accuracy, precision, recall, F1-score, and ROC-AUC**.\n",
    "5. Display the **confusion matrix** and comment on performance.\n",
    "6. Reflect: What challenges did you face in image-based fraud detection?\n",
    "\n",
    "---\n",
    "\n",
    "### Option B: Transfer Learning with Pretrained Model (e.g., MobileNetV2)\n",
    "\n",
    "1. Use `ImageDataGenerator` or another loader to prepare image batches.\n",
    "2. Load a pretrained image classification model (e.g., MobileNetV2, ResNet50).\n",
    "3. Fine-tune the model using your deepfake dataset.\n",
    "4. Evaluate performance using the same metrics as Option A.\n",
    "5. Comment on whether transfer learning helped and why.\n",
    "6. Reflect: What surprised you about deepfake detection performance?\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Deliverables\n",
    "\n",
    "Your submission should include:\n",
    "\n",
    "- A complete Jupyter Notebook with:\n",
    "  - Code and visualizations\n",
    "  - Markdown explanations\n",
    "  - Confusion matrix and metric outputs\n",
    "  - A **3–5 sentence reflection** on modeling approach and results\n",
    "\n",
    "---\n",
    "\n",
    "## ⭐ Bonus (Optional – 50 Points Extra Credit)\n",
    "\n",
    "**Compare CNN performance to a baseline model using metadata only.**\n",
    "\n",
    "1. Use `train.csv` to build a tabular model using only metadata (no image data).\n",
    "2. Train a **Logistic Regression** or **Random Forest** classifier.\n",
    "3. Evaluate and compare the tabular model to your image-based model.\n",
    "4. Reflect on which method performed better and why.\n",
    "5. Comment on how metadata can complement or substitute deep learning in fraud analytics.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Grading Rubric (Main Assignment – 100 Points)\n",
    "\n",
    "| Component                                 | Points |\n",
    "|------------------------------------------|--------|\n",
    "| Data pipeline: image loading + preprocessing | 20     |\n",
    "| Model construction (CNN or transfer model) | 25     |\n",
    "| Evaluation: confusion matrix + metrics   | 20     |\n",
    "| Visualizations and architecture summaries | 15     |\n",
    "| Markdown clarity + comments              | 10     |\n",
    "| Final reflection                         | 10     |\n",
    "| **Total**                                | **100**|\n",
    "\n",
    "---\n",
    "\n",
    "## ➕ Extra Credit Rubric (Optional – 50 Points)\n",
    "\n",
    "| Component                                          | Points |\n",
    "|---------------------------------------------------|--------|\n",
    "| Metadata-based model constructed and evaluated    | 20     |\n",
    "| Performance comparison and insight                | 15     |\n",
    "| Clarity and depth of final reflection             | 15     |\n",
    "| **Total**                                         | **50** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746fc4d-5143-44f3-98d4-3a345e09c3e1",
   "metadata": {
    "id": "2746fc4d-5143-44f3-98d4-3a345e09c3e1"
   },
   "source": [
    "## 📥 Getting Started: Load and Prepare the Deepfake Image Dataset\n",
    "\n",
    "Before we build any machine learning models, we need to load and preprocess our image data.\n",
    "\n",
    "In this starter code, you'll:\n",
    "- Load images from separate folders (`real` and `fake`)\n",
    "- Resize each image to a standard size (128x128)\n",
    "- Normalize pixel values to the [0, 1] range\n",
    "- Assign class labels (`0 = real`, `1 = fake`)\n",
    "- Combine the images into a unified dataset for modeling\n",
    "\n",
    "You’ll be working with a simplified version of a real deepfake detection dataset to build and test your model. We’ve provided a helper function to streamline image loading, so you can focus on model development and evaluation.\n",
    "\n",
    "Run the code below to load the dataset into memory. Once it completes successfully, you’ll see the dimensions of your image data (`X`) and label array (`y`), which will serve as inputs to your deepfake detection model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb11e560-915e-4ad5-8cd3-e3d6f85bf441",
   "metadata": {
    "id": "bb11e560-915e-4ad5-8cd3-e3d6f85bf441",
    "outputId": "898c1e84-dbe8-4a58-9d05-252bb43689b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dataset shape: (1000, 224, 224, 3)\n",
      "Label array shape: (1000,)\n",
      "\n",
      "Training data shape: (800, 224, 224, 3)\n",
      "Testing data shape: (200, 224, 224, 3)\n",
      "Training labels shape: (800,)\n",
      "Testing labels shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os                        # Used for navigating file directories\n",
    "import numpy as np               # Used for numerical operations and array management\n",
    "from PIL import Image            # Used to open, manipulate, and resize image files\n",
    "import matplotlib.pyplot as plt  # Used for displaying sample images\n",
    "\n",
    "# Define file paths to the real and fake image folders\n",
    "real_path = os.path.expanduser('~/Documents/archive/real_vs_fake/real-vs-fake/train/real')\n",
    "fake_path = os.path.expanduser('~/Documents/archive/real_vs_fake/real-vs-fake/train/fake')\n",
    "\n",
    "# Define a function to load and preprocess images from a given folder\n",
    "# Updated function \"img_size=(224, 224)\" for model training\n",
    "def load_images(folder_path, label, img_size=(224, 224), limit=500):\n",
    "    \"\"\"\n",
    "    Loads images from a folder, resizes them, and assigns a label.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): The path to the image directory\n",
    "        label (int): The class label for the images (0 for real, 1 for fake)\n",
    "        img_size (tuple): Target size to resize images to (width, height)\n",
    "        limit (int): Max number of images to load from the folder\n",
    "\n",
    "    Returns:\n",
    "        images (list): A list of preprocessed image arrays\n",
    "        labels (list): A list of labels corresponding to each image\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Get all image files and shuffle them\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    np.random.shuffle(files)\n",
    "\n",
    "    # Loop through shuffled files\n",
    "    for i, file_name in enumerate(files):\n",
    "        file_path = os.path.join(folder_path, file_name)  # Full file path\n",
    "        img = Image.open(file_path).convert('RGB')        # Open and convert to RGB format\n",
    "        img = img.resize(img_size)                        # Resize image to target size\n",
    "        images.append(np.array(img) / 255.0)              # Normalize pixel values to range [0, 1]\n",
    "        labels.append(label)                              # Assign label to this image\n",
    "\n",
    "        # Stop loading if the specified limit is reached\n",
    "        if i >= limit - 1:\n",
    "            break\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Load and label real and fake images using the helper function\n",
    "real_images, real_labels = load_images(real_path, label=0)\n",
    "fake_images, fake_labels = load_images(fake_path, label=1)\n",
    "\n",
    "# Combine the real and fake datasets into a single dataset\n",
    "X = np.array(real_images + fake_images)  # Feature matrix containing image data\n",
    "y = np.array(real_labels + fake_labels)  # Target labels: 0 for real, 1 for fake\n",
    "\n",
    "# Display the shape of the dataset to confirm it loaded correctly\n",
    "print(\"Image dataset shape:\", X.shape)  # Should now show (1000, 224, 224, 3)\n",
    "print(\"Label array shape:\", y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split   # Used to split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Testing labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f8a2e2-f1bd-4343-90fa-c19fb6136f69",
   "metadata": {
    "id": "41a915cb-15b9-4d47-a623-2ee779f69dbd"
   },
   "source": [
    "# **Observations - Dataset (Kaggle Files & Folders)**\n",
    "- Dataset\n",
    "    - Large (4 GB)!\n",
    "        - Selected 1,000 Images at random from possible 140,000 Total Images\n",
    "            - Selected 500 Real v. 500 Fake Images\n",
    "        - Unarchived Files & Folders saved to \"~/Documents/\" (MacOS)\n",
    "        - Chose to save locally instead of Google Drive due to cloud memory and latency considerations\n",
    "    - Resized to 224x224 pixels to allow for MobileNetV2/CNN training (up from original resolution)\n",
    "    - Training/Testing split = 80/20 (800 Images for Training/200 Images for Testing)\n",
    "    - Image preprocessing: Normalized pixel values to [0,1] range, converted to RGB format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8EcG5vDUjluT",
   "metadata": {
    "id": "8EcG5vDUjluT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.73.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "\n",
      "TensorFlow version: 2.19.0\n",
      "\n",
      "✅ Model compiled. Total parameters: 2,259,265\n"
     ]
    }
   ],
   "source": [
    "# Install TensorFlow\n",
    "!pip install tensorflow\n",
    "\n",
    "# Then restart kernel and run:\n",
    "import tensorflow as tf\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# =====================================\n",
    "# TRANSFER LEARNING MODEL (MobileNetV2)\n",
    "# =====================================\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load pre-trained MobileNetV2 (exclude top layer)\n",
    "base_model = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom classification head\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "print(f\"\\n✅ Model compiled. Total parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee1b86-8ae5-4d9b-9e42-8cc9227c12ac",
   "metadata": {},
   "source": [
    "# **Observations - Transfer Learning Model (MobileNetV2)**\n",
    "- Transfer Learning Model\n",
    "    - Chose MobileNetV2 model: pre-trained in Image recognition (ImageNet weights) with frozen layers\n",
    "        - 2.259M+ total parameters\n",
    "        - Model Configuration: Adam learning method, binary loss measurement for real/fake classification, tracking accuracy percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc673f85-448f-4aef-8da0-9b138c4e718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current data shapes:\n",
      "X_train: (800, 224, 224, 3)\n",
      "X_test: (200, 224, 224, 3)\n",
      "Training labels: Real=400, Fake=400\n",
      "Test labels: Real=100, Fake=100\n",
      "\n",
      "Training transfer learning model...\n",
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.5160 - loss: 0.7511 - val_accuracy: 0.6100 - val_loss: 0.6812\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 195ms/step - accuracy: 0.5961 - loss: 0.6737 - val_accuracy: 0.6550 - val_loss: 0.6446\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 189ms/step - accuracy: 0.6296 - loss: 0.6579 - val_accuracy: 0.6000 - val_loss: 0.6651\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 187ms/step - accuracy: 0.6610 - loss: 0.6209 - val_accuracy: 0.6350 - val_loss: 0.6126\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 181ms/step - accuracy: 0.7276 - loss: 0.5743 - val_accuracy: 0.6750 - val_loss: 0.6284\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 180ms/step - accuracy: 0.7032 - loss: 0.5719 - val_accuracy: 0.6650 - val_loss: 0.5940\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 180ms/step - accuracy: 0.7429 - loss: 0.5114 - val_accuracy: 0.6800 - val_loss: 0.6015\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 180ms/step - accuracy: 0.6926 - loss: 0.5580 - val_accuracy: 0.6600 - val_loss: 0.5753\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 183ms/step - accuracy: 0.7601 - loss: 0.5014 - val_accuracy: 0.6850 - val_loss: 0.5928\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 181ms/step - accuracy: 0.7611 - loss: 0.4940 - val_accuracy: 0.6850 - val_loss: 0.5686\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step\n",
      "\n",
      "✅ Model training complete!\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# DATA PREPARATION & SPLITTING\n",
    "# =====================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Your original code created X_train, X_test, y_train, y_test already\n",
    "# But we need to resize images for MobileNetV2 (224x224 instead of 224x224)\n",
    "print(f\"Current data shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "\n",
    "# Images should already be the right size (224, 224, 3) from your original code\n",
    "# If not, you may need to resize them\n",
    "\n",
    "print(f\"Training labels: Real={np.sum(y_train==1)}, Fake={np.sum(y_train==0)}\")\n",
    "print(f\"Test labels: Real={np.sum(y_test==1)}, Fake={np.sum(y_test==0)}\")\n",
    "\n",
    "# =====================================\n",
    "# TRAIN MODEL\n",
    "# =====================================\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "print(\"\\nTraining transfer learning model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\n✅ Model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "896266c2-18c4-4a90-9860-fc123cb46840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# **Observations - Model Training Process**\n",
       "- Data Verification\n",
       "   - Training set: 800 images (224x224x3), balanced 400 real/400 fake\n",
       "   - Test set: 200 images (224x224x3), balanced 100 real/100 fake\n",
       "   - Images already properly sized for MobileNetV2 (224x224 pixels)\n",
       "- Training Configuration\n",
       "   - 10 Epochs max: Prevents overfitting while allowing sufficient learning time for transfer learning\n",
       "       - **Insight:** Less memorization, More learning \n",
       "   - Batch Size = 32: Balances memory efficiency with stable gradient updates (800/32 = 25 Batches per Epoch)\n",
       "       - **Insight:** Better to update learning after 10+ but less than 50 \n",
       "   - \"early_stopping\": Automatically halts training if validation loss stops improving for 3 consecutive Epochs\n",
       "       - **Insight:** Model Training continued to improve until Max Epoch reached \n",
       "- Training Results\n",
       "   - Model completed all 10 Epochs without early stoppage\n",
       "   - Final Validation Accuracy (\"val_accuracy\"): 68.5%\n",
       "       - **Insight:** Training showed steady improvement from 61.0% to 68.5% Validation Accuracy\n",
       "   - Final Validation Loss (\"val_loss\"): 0.569\n",
       "       - **Insight:** Validation loss decreased from 0.681 to 0.569 indicating effective learning\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Extract dynamic values from training history\n",
    "final_val_accuracy = history.history['val_accuracy'][-1]\n",
    "initial_val_accuracy = history.history['val_accuracy'][0]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "initial_val_loss = history.history['val_loss'][0]\n",
    "\n",
    "markdown_content = f\"\"\"\n",
    "# **Observations - Model Training Process**\n",
    "- Data Verification\n",
    "   - Training set: 800 images (224x224x3), balanced 400 real/400 fake\n",
    "   - Test set: 200 images (224x224x3), balanced 100 real/100 fake\n",
    "   - Images already properly sized for MobileNetV2 (224x224 pixels)\n",
    "- Training Configuration\n",
    "   - 10 Epochs max: Prevents overfitting while allowing sufficient learning time for transfer learning\n",
    "       - **Insight:** Less memorization, More learning \n",
    "   - Batch Size = 32: Balances memory efficiency with stable gradient updates (800/32 = 25 Batches per Epoch)\n",
    "       - **Insight:** Better to update learning after 10+ but less than 50 \n",
    "   - \"early_stopping\": Automatically halts training if validation loss stops improving for 3 consecutive Epochs\n",
    "       - **Insight:** Model Training continued to improve until Max Epoch reached \n",
    "- Training Results\n",
    "   - Model completed all 10 Epochs without early stoppage\n",
    "   - Final Validation Accuracy (\"val_accuracy\"): {final_val_accuracy:.1%}\n",
    "       - **Insight:** Training showed steady improvement from {initial_val_accuracy:.1%} to {final_val_accuracy:.1%} Validation Accuracy\n",
    "   - Final Validation Loss (\"val_loss\"): {final_val_loss:.3f}\n",
    "       - **Insight:** Validation loss decreased from {initial_val_loss:.3f} to {final_val_loss:.3f} indicating effective learning\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5aa834-9b61-4979-afc9-e0c2a0819d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## MobileNetV2 MODEL EVALUATION & VISUALIZATION"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TRANSFER LEARNING RESULTS\n",
      "==================================================\n",
      "Precision: 0.670\n",
      "Recall: 0.730\n",
      "F1-Score: 0.699\n",
      "ROC-AUC: 0.777\n",
      "\n",
      "Confusion Matrix:\n",
      "[[64 36]\n",
      " [27 73]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.70      0.64      0.67       100\n",
      "        Fake       0.67      0.73      0.70       100\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.69      0.69      0.68       200\n",
      "weighted avg       0.69      0.69      0.68       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# MODEL EVALUATION\n",
    "# =====================================\n",
    "\n",
    "# Import Markdown\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "display(Markdown(\"## MobileNetV2 MODEL EVALUATION & VISUALIZATION\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRANSFER LEARNING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))\n",
    "\n",
    "# Simple visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "metrics = ['Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "values = [precision, recall, f1, roc_auc]\n",
    "plt.bar(metrics, values, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Model Performance')\n",
    "plt.ylabel('Score')\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a3b93cc-3e35-4392-a0ab-36f81aa2cf7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# **Observations - MobileNetV2 Model Results & Analysis**\n",
       "- **Performance Breakdown:**\n",
       "   - **True Negatives** (Correctly identified as Real): 64\n",
       "   - **False Positives** (Incorrectly identified as Fake): 36\n",
       "   - **False Negatives** (Incorrectly identified as Real): 27\n",
       "   - **True Positives** (Correctly identified as Fake): 73\n",
       "## **Performance Metrics**\n",
       "- \"Precision\" = 0.670 = True Positive/(True Positive + False Positive)\n",
       "  - Of all images flagged as Fake, 67.0% were actually Fake; 33.0% were falsely flagged (false positives)\n",
       "- \"Recall\" = 0.730 = True Positive/(True Positive + False Negative)\n",
       "  - Of all Fake images, 73% were correctly identified as Fake; 27% of Fake images were not identified (false negatives)\n",
       "- \"F-1 Score\" = 0.699 = 2 * (Precision * Recall)/(Precision + Recall)\n",
       "- \"ROC-AUC\" = 77.7%\n",
       "- **Insight:** Both Precision and Recall are reasonably balanced around 67%-73%, indicating stable performance across both classes\n",
       "- **Insight:** ROC-AUC > 0.8 shows good ability to distinguish between Real and Fake images - significantly better than random guessing\n",
       "- **Insight:** Model performs slightly better at detecting Fakes (73% Recall) than avoiding false alarms (67.0% Precision)\n",
       "   - **Insight:** 27% false negative rate means model **missed** ~1 in 4 Deepfakes - concerning for security applications\n",
       "   - **Insight:** 36% false positive rate means ~1 in 3 Real images **incorrectly flagged** as Fake - potential user experience issue\n",
       "## **Detailed Classification Report**\n",
       "- \"Support\" = Actual count of each image class in test set: 100 Real images v. 100 Fake images\n",
       "- \"Macro avg\" = Simple average across both classes (67% Precision, 73% Recall)\n",
       "  - Treats Real and Fake images equally, showing balanced model performance\n",
       "- \"Weighted avg\" = Size-weighted average (67% Precision, 73% Recall)\n",
       "  - Equal class sizes result in identical macro and weighted averages\n",
       "      - **Insight:** Balanced Dataset eliminates bias in evaluation metrics\n",
       "## **Confusion Matrix**\n",
       "- Model incorrectly identified 27 Fake images as Real images\n",
       "  - **Moderate** rate (27%) of False Negatives = Security Risk\n",
       "- Model incorrectly identified 36 Real images as Fake images\n",
       "  - **Moderate** rate (36%) of False Positives = User Experience Concern\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Extract dynamic values from model evaluation\n",
    "tn = cm[0, 0]  # True Negatives\n",
    "fp = cm[0, 1]  # False Positives  \n",
    "fn = cm[1, 0]  # False Negatives\n",
    "tp = cm[1, 1]  # True Positives\n",
    "\n",
    "false_negative_rate = fn / (tp + fn)\n",
    "false_positive_rate = fp / (tn + fp)\n",
    "\n",
    "markdown_content = f\"\"\"\n",
    "# **Observations - MobileNetV2 Model Results & Analysis**\n",
    "- **Performance Breakdown:**\n",
    "   - **True Negatives** (Correctly identified as Real): {tn}\n",
    "   - **False Positives** (Incorrectly identified as Fake): {fp}\n",
    "   - **False Negatives** (Incorrectly identified as Real): {fn}\n",
    "   - **True Positives** (Correctly identified as Fake): {tp}\n",
    "## **Performance Metrics**\n",
    "- \"Precision\" = {precision:.3f} = True Positive/(True Positive + False Positive)\n",
    "  - Of all images flagged as Fake, {precision:.1%} were actually Fake; {1-precision:.1%} were falsely flagged (false positives)\n",
    "- \"Recall\" = {recall:.3f} = True Positive/(True Positive + False Negative)\n",
    "  - Of all Fake images, {recall:.0%} were correctly identified as Fake; {1-recall:.0%} of Fake images were not identified (false negatives)\n",
    "- \"F-1 Score\" = {f1:.3f} = 2 * (Precision * Recall)/(Precision + Recall)\n",
    "- \"ROC-AUC\" = {roc_auc:.1%}\n",
    "- **Insight:** Both Precision and Recall are reasonably balanced around {precision:.0%}-{recall:.0%}, indicating stable performance across both classes\n",
    "- **Insight:** ROC-AUC > 0.8 shows good ability to distinguish between Real and Fake images - significantly better than random guessing\n",
    "- **Insight:** Model performs slightly better at detecting Fakes ({recall:.0%} Recall) than avoiding false alarms ({precision:.1%} Precision)\n",
    "   - **Insight:** {false_negative_rate:.0%} false negative rate means model **missed** ~1 in {1/false_negative_rate:.0f} Deepfakes - concerning for security applications\n",
    "   - **Insight:** {false_positive_rate:.0%} false positive rate means ~1 in {1/false_positive_rate:.0f} Real images **incorrectly flagged** as Fake - potential user experience issue\n",
    "## **Detailed Classification Report**\n",
    "- \"Support\" = Actual count of each image class in test set: 100 Real images v. 100 Fake images\n",
    "- \"Macro avg\" = Simple average across both classes ({precision:.0%} Precision, {recall:.0%} Recall)\n",
    "  - Treats Real and Fake images equally, showing balanced model performance\n",
    "- \"Weighted avg\" = Size-weighted average ({precision:.0%} Precision, {recall:.0%} Recall)\n",
    "  - Equal class sizes result in identical macro and weighted averages\n",
    "      - **Insight:** Balanced Dataset eliminates bias in evaluation metrics\n",
    "## **Confusion Matrix**\n",
    "- Model incorrectly identified {fn} Fake images as Real images\n",
    "  - **Moderate** rate ({false_negative_rate:.0%}) of False Negatives = Security Risk\n",
    "- Model incorrectly identified {fp} Real images as Fake images\n",
    "  - **Moderate** rate ({false_positive_rate:.0%}) of False Positives = User Experience Concern\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc05699-9c32-4eab-84b1-699857613a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN...\n",
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 472ms/step - accuracy: 0.4826 - loss: 1.8280 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 519ms/step - accuracy: 0.5295 - loss: 0.6904 - val_accuracy: 0.5000 - val_loss: 0.7020\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 503ms/step - accuracy: 0.5818 - loss: 0.6806 - val_accuracy: 0.5800 - val_loss: 0.6795\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 498ms/step - accuracy: 0.7069 - loss: 0.6021 - val_accuracy: 0.5550 - val_loss: 0.6947\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 494ms/step - accuracy: 0.6934 - loss: 0.6114 - val_accuracy: 0.5700 - val_loss: 0.7110\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 489ms/step - accuracy: 0.7314 - loss: 0.5399 - val_accuracy: 0.5500 - val_loss: 0.9307\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 484ms/step - accuracy: 0.7861 - loss: 0.4438 - val_accuracy: 0.6100 - val_loss: 0.8397\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 492ms/step - accuracy: 0.8540 - loss: 0.3407 - val_accuracy: 0.5750 - val_loss: 0.9472\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 494ms/step - accuracy: 0.8928 - loss: 0.2702 - val_accuracy: 0.5950 - val_loss: 1.2315\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 485ms/step - accuracy: 0.9463 - loss: 0.1641 - val_accuracy: 0.6000 - val_loss: 1.4281\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 114ms/step\n",
      "\n",
      "CNN RESULTS\n",
      "====================\n",
      "Precision: 0.583\n",
      "Recall: 0.700\n",
      "F1-Score: 0.636\n",
      "ROC-AUC: 0.635\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50 50]\n",
      " [30 70]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.62      0.50      0.56       100\n",
      "        Real       0.58      0.70      0.64       100\n",
      "\n",
      "    accuracy                           0.60       200\n",
      "   macro avg       0.60      0.60      0.60       200\n",
      "weighted avg       0.60      0.60      0.60       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# EXTRA CREDIT: METADATA-ONLY MODEL\n",
    "# CNN MODEL\n",
    "# =====================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Build basic CNN with Input layer\n",
    "cnn_model = Sequential([\n",
    "    Input(shape=(224, 224, 3)),  # Add Input layer first\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "\n",
    "cnn_model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train CNN\n",
    "print(\"Training CNN...\")\n",
    "history = cnn_model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                   epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate CNN\n",
    "y_pred_proba = cnn_model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"\\nCNN RESULTS\")\n",
    "print(\"=\"*20)\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, target_names=['Fake', 'Real']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b7f45a1-d4fc-4403-a107-511fbe180f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# **EXTRA CREDIT**\n",
       "# **Observations - CNN Model Training Process**\n",
       "**Data Verification**\n",
       "   - Training set: 800 images (224x224x3), balanced 400 real/400 fake\n",
       "   - Test set: 200 images (224x224x3), balanced 100 real/100 fake\n",
       "   - Images properly sized for CNN Model (224x224 pixels)\n",
       "- Training Configuration\n",
       "   - 10 Epochs max: Full training duration without early stopping\n",
       "        - **Insight:** Less memorization, More learning \n",
       "        - **Insight:** Model trained through complete Epoch cycle\n",
       "   - Batch Size = 32: Balances memory efficiency with stable gradient updates (800/32 = 25 Batches per Epoch)\n",
       "        - **Insight:** Better to update learning after 10+ but less than 50 \n",
       "        - **Insight:** Standard batch size for image classification tasks\n",
       "   - Custom CNN Architecture: 3 Conv layers (32→64→128 filters) + Dense(512) + Dropout(0.5)\n",
       "        - **Insight:** Progressive feature extraction from basic to complex patterns\n",
       "- **Training Results**\n",
       "    - Model completed all 10 Epochs\n",
       "    - Final Validation Accuracy: 60.0%\n",
       "        - **Insight:** Validation accuracy improved from 50.0% to 60.0%, showing improving detection\n",
       "    - Final Validation Loss: 1.428\n",
       "        - **Insight:** Validation loss increased from 0.693 to 1.428, showing distressing predictions\n",
       "    - **Insight:** Improved detection but increased uncertainty in flagging Fake images, meaning CNN Model is **not learning** and is **instead memorizing** patterns\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Extract dynamic values from CNN training history\n",
    "final_val_accuracy = history.history['val_accuracy'][-1]\n",
    "initial_val_accuracy = history.history['val_accuracy'][0]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "initial_val_loss = history.history['val_loss'][0]\n",
    "\n",
    "# Determine if loss improved or worsened\n",
    "loss_direction = \"decreased\" if final_val_loss < initial_val_loss else \"increased\"\n",
    "\n",
    "markdown_content = f\"\"\"\n",
    "# **EXTRA CREDIT**\n",
    "# **Observations - CNN Model Training Process**\n",
    "**Data Verification**\n",
    "   - Training set: 800 images (224x224x3), balanced 400 real/400 fake\n",
    "   - Test set: 200 images (224x224x3), balanced 100 real/100 fake\n",
    "   - Images properly sized for CNN Model (224x224 pixels)\n",
    "- Training Configuration\n",
    "   - 10 Epochs max: Full training duration without early stopping\n",
    "        - **Insight:** Less memorization, More learning \n",
    "        - **Insight:** Model trained through complete Epoch cycle\n",
    "   - Batch Size = 32: Balances memory efficiency with stable gradient updates (800/32 = 25 Batches per Epoch)\n",
    "        - **Insight:** Better to update learning after 10+ but less than 50 \n",
    "        - **Insight:** Standard batch size for image classification tasks\n",
    "   - Custom CNN Architecture: 3 Conv layers (32→64→128 filters) + Dense(512) + Dropout(0.5)\n",
    "        - **Insight:** Progressive feature extraction from basic to complex patterns\n",
    "- **Training Results**\n",
    "    - Model completed all 10 Epochs\n",
    "    - Final Validation Accuracy: {final_val_accuracy:.1%}\n",
    "        - **Insight:** Validation accuracy improved from {initial_val_accuracy:.1%} to {final_val_accuracy:.1%}, showing improving detection\n",
    "    - Final Validation Loss: {final_val_loss:.3f}\n",
    "        - **Insight:** Validation loss {loss_direction} from {initial_val_loss:.3f} to {final_val_loss:.3f}, showing distressing predictions\n",
    "    - **Insight:** Improved detection but increased uncertainty in flagging Fake images, meaning CNN Model is **not learning** and is **instead memorizing** patterns\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753759a5-a902-4744-87bf-3ecf9856a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV shape: (100000, 6)\n",
      "Columns: ['Unnamed: 0', 'original_path', 'id', 'label', 'label_str', 'path']\n",
      "\n",
      "   Unnamed: 0                                      original_path     id  \\\n",
      "0           0  /kaggle/input/flickrfaceshq-dataset-nvidia-par...  31355   \n",
      "1           1  /kaggle/input/flickrfaceshq-dataset-nvidia-par...  02884   \n",
      "2           2  /kaggle/input/flickrfaceshq-dataset-nvidia-par...  33988   \n",
      "3           3  /kaggle/input/flickrfaceshq-dataset-nvidia-par...  53875   \n",
      "4           4  /kaggle/input/flickrfaceshq-dataset-nvidia-par...  24149   \n",
      "\n",
      "   label label_str                  path  \n",
      "0      1      real  train/real/31355.jpg  \n",
      "1      1      real  train/real/02884.jpg  \n",
      "2      1      real  train/real/33988.jpg  \n",
      "3      1      real  train/real/53875.jpg  \n",
      "4      1      real  train/real/24149.jpg  \n",
      "\n",
      "Metadata features shape: (100000, 3)\n",
      "Feature columns: ['filename_length', 'has_numbers', 'file_extension_encoded']\n",
      "\n",
      "BONUS: METADATA vs IMAGE MODEL COMPARISON\n",
      "=============================================\n",
      "Image-based CNN ROC-AUC: 0.635\n",
      "Metadata-only RF ROC-AUC: 1.000\n",
      "Better performer: Metadata\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# EXTRA CREDIT: METADATA-ONLY MODEL\n",
    "# CNN MODEL v. METADATA-ONLY MODEL COMPARISON\n",
    "# =====================================\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Load metadata CSV\n",
    "train_csv_path = os.path.expanduser('~/Documents/archive/train.csv')\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "\n",
    "print(f\"CSV shape: {train_df.shape}\")\n",
    "print(f\"Columns: {train_df.columns.tolist()}\")\n",
    "print()\n",
    "print(train_df.head())\n",
    "\n",
    "# 2. Create numeric features from metadata (no image data)\n",
    "metadata_features = pd.DataFrame()\n",
    "\n",
    "# Extract features from file paths/names\n",
    "if 'path' in train_df.columns:\n",
    "    metadata_features['filename_length'] = train_df['path'].str.len()\n",
    "    metadata_features['has_numbers'] = train_df['path'].str.contains(r'\\d').astype(int)\n",
    "    metadata_features['file_extension'] = train_df['path'].str.split('.').str[-1]\n",
    "    \n",
    "    # Encode file extension\n",
    "    le = LabelEncoder()\n",
    "    metadata_features['file_extension_encoded'] = le.fit_transform(metadata_features['file_extension'])\n",
    "    metadata_features = metadata_features.drop('file_extension', axis=1)\n",
    "\n",
    "# If image_id exists, create features from it\n",
    "if 'image_id' in train_df.columns:\n",
    "    metadata_features['id_length'] = train_df['image_id'].astype(str).str.len()\n",
    "\n",
    "# Ensure we have some features\n",
    "if metadata_features.empty:\n",
    "    # Create simple index-based features as fallback\n",
    "    metadata_features['row_index'] = range(len(train_df))\n",
    "    metadata_features['label_encoded'] = train_df['label']\n",
    "\n",
    "print(f\"\\nMetadata features shape: {metadata_features.shape}\")\n",
    "print(f\"Feature columns: {metadata_features.columns.tolist()}\")\n",
    "\n",
    "# Sample same size as image dataset (1000)\n",
    "sample_size = min(1000, len(train_df))\n",
    "sample_indices = np.random.choice(len(train_df), size=sample_size, replace=False)\n",
    "\n",
    "X_metadata = metadata_features.iloc[sample_indices]\n",
    "y_metadata = train_df['label'].iloc[sample_indices]\n",
    "\n",
    "# Split metadata\n",
    "X_meta_train, X_meta_test, y_meta_train, y_meta_test = train_test_split(\n",
    "    X_metadata, y_metadata, test_size=0.2, random_state=42, stratify=y_metadata)\n",
    "\n",
    "# 3. Train Random Forest classifier\n",
    "rf_metadata = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_metadata.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Evaluate metadata model\n",
    "meta_auc = roc_auc_score(y_meta_test, rf_metadata.predict_proba(X_meta_test)[:, 1])\n",
    "\n",
    "# 4. Compare models\n",
    "print(\"\\nBONUS: METADATA vs IMAGE MODEL COMPARISON\")\n",
    "print(\"=\"*45)\n",
    "print(f\"Image-based CNN ROC-AUC: {roc_auc:.3f}\")\n",
    "print(f\"Metadata-only RF ROC-AUC: {meta_auc:.3f}\")\n",
    "print(f\"Better performer: {'CNN' if roc_auc > meta_auc else 'Metadata'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c705f29-8fc9-4cc3-beed-5f3afe455cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# **EXTRA CREDIT**\n",
       "# **Observations - Metadata-Only Model vs Image-Based Models**\n",
       "- **Metadata Feature Engineering**\n",
       "   - Extracted features from file paths: filename length, presence of numbers, file extension\n",
       "   - Used Random Forest classifier with 100 estimators\n",
       "   - Same dataset split (800 train/200 test) for fair comparison\n",
       "   - Features based on file metadata rather than actual image content\n",
       "- **Metadata Model Performance**\n",
       "   - Random Forest ROC-AUC: 1.000\n",
       "   - Training on filename patterns and file characteristics only\n",
       "   - **Insight:** Suspiciously high performance suggests Dataset file naming patterns accidentally reveal the answers (either by Fraudsters or Dataset Researchers)\n",
       "- **Model Comparison Results**\n",
       "   - CNN from Scratch ROC-AUC: 0.635\n",
       "   - Metadata-only RF ROC-AUC: 1.000\n",
       "   - Better performer: Metadata\n",
       "- **Key Insights - CNN vs MobileNetV2 vs Metadata:**\n",
       "    - **CNN Learning Limitation:** CNN Model struggled with feature extraction from scratch, achieving moderate performance but showing overfitting behavior \n",
       "        - **Insight:** CNN Model improving accuracy but with worsening loss\n",
       "    - **Transfer Learning Advantage:** MobileNetV2 leveraged pre-trained ImageNet features for **superior Deepfake detection**\n",
       "        - **Insight:** MobileNetV2 Model completed training in half the time it took CNN Model\n",
       "        - **Insight:** Established visual pattern recognition (MobileNetV2) > Building from scratch (CNN)\n",
       "    - **Metadata Anomaly:** Perfect/near-perfect Metadata performance likely indicates easily identifiable Dataset patterns rather than any real accuracy\n",
       "        - **Insight:** While Metadata could be useful in detection, more complex Datasets could fool models strictly using Metadata instead of real training\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Extract metadata model performance\n",
    "# meta_auc should be available from the metadata model evaluation\n",
    "# roc_auc should be available from the CNN model evaluation\n",
    "\n",
    "markdown_content = f\"\"\"\n",
    "# **EXTRA CREDIT**\n",
    "# **Observations - Metadata-Only Model vs Image-Based Models**\n",
    "- **Metadata Feature Engineering**\n",
    "   - Extracted features from file paths: filename length, presence of numbers, file extension\n",
    "   - Used Random Forest classifier with 100 estimators\n",
    "   - Same dataset split (800 train/200 test) for fair comparison\n",
    "   - Features based on file metadata rather than actual image content\n",
    "- **Metadata Model Performance**\n",
    "   - Random Forest ROC-AUC: {meta_auc:.3f}\n",
    "   - Training on filename patterns and file characteristics only\n",
    "   - **Insight:** Suspiciously high performance suggests Dataset file naming patterns accidentally reveal the answers (either by Fraudsters or Dataset Researchers)\n",
    "- **Model Comparison Results**\n",
    "   - CNN from Scratch ROC-AUC: {roc_auc:.3f}\n",
    "   - Metadata-only RF ROC-AUC: {meta_auc:.3f}\n",
    "   - Better performer: {'CNN' if roc_auc > meta_auc else 'Metadata'}\n",
    "- **Key Insights - CNN vs MobileNetV2 vs Metadata:**\n",
    "    - **CNN Learning Limitation:** CNN Model struggled with feature extraction from scratch, achieving moderate performance but showing overfitting behavior \n",
    "        - **Insight:** CNN Model improving accuracy but with worsening loss\n",
    "    - **Transfer Learning Advantage:** MobileNetV2 leveraged pre-trained ImageNet features for **superior Deepfake detection**\n",
    "        - **Insight:** MobileNetV2 Model completed training in half the time it took CNN Model\n",
    "        - **Insight:** Established visual pattern recognition (MobileNetV2) > Building from scratch (CNN)\n",
    "    - **Metadata Anomaly:** Perfect/near-perfect Metadata performance likely indicates easily identifiable Dataset patterns rather than any real accuracy\n",
    "        - **Insight:** While Metadata could be useful in detection, more complex Datasets could fool models strictly using Metadata instead of real training\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b55af6-e641-4f01-8476-4041bc277f10",
   "metadata": {},
   "source": [
    "# **THANK YOU JAMES FOR A GREAT CLASS!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5568ade-f7b2-49f4-b632-320cd9ecc413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
